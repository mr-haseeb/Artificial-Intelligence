{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with TensorFlow Lite\n",
    "TensorFlow Lite provides all the tools you need to convert and run TensorFlow models on mobile, embedded, and IoT devices. The following guide walks through each step of the developer workflow and provides links to further instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.tensorflow.org/lite/examples\n",
    "* https://www.tensorflow.org/lite/guide/get_started\n",
    "* <b>Must Checkout</b>\n",
    "* https://www.tensorflow.org/lite/convert/python_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a pre-trained model\n",
    "The TensorFlow Lite team provides a set of pre-trained models that solve a variety of machine learning problems. These models have been converted to work with TensorFlow Lite and are ready to use in your applications.\n",
    "\n",
    "The pre-trained models include:\n",
    "\n",
    "1. Image classification\n",
    "2. Object detection\n",
    "3. Smart reply\n",
    "4. Pose estimation\n",
    "5. Segmentation\n",
    "<br>\n",
    "<b>See our full list of pre-trained models in Models.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device deployment\n",
    "The TensorFlow Lite FlatBuffer file is then deployed to a client device (e.g. mobile, embedded) and run locally using the TensorFlow Lite interpreter. This conversion process is shown in the diagram below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"workflow.svg\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Using SaveModel (dir contain .pb and meta file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SavedModel being converted into the TensorFlow Lite format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "By reducing the precision of values and operations within a model, quantization can reduce both the size of model and the time required for inference. For many models, there is only a minimal loss of accuracy.\n",
    "\n",
    "The TensorFlow Lite converter makes it easy to quantize TensorFlow models. The following Python code quantizes a SavedModel and saves it to disk:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "open(\"converted_model.tflite\", \"wb\").write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Construct a basic model.\n",
    "root = tf.train.Checkpoint()\n",
    "root.v1 = tf.Variable(3.)\n",
    "root.v2 = tf.Variable(2.)\n",
    "root.f = tf.function(lambda x: root.v1 * root.v2 * x)\n",
    "\n",
    "# Save the model.\n",
    "export_dir = \"/tmp/test_saved_model\"\n",
    "input_data = tf.constant(1., shape=[1, 1])\n",
    "to_save = root.f.get_concrete_function(input_data)\n",
    "tf.saved_model.save(root, export_dir, to_save)\n",
    "\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Manual input Shape for Image \n",
    "This API does not have the option of specifying the input shape of any input arrays. If your model requires specifying the input shape, use the from_concrete_functions classmethod instead. The code looks similar to the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(export_dir)\n",
    "concrete_func = model.signatures[\n",
    "tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "concrete_func.inputs[0].set_shape([1, 256, 256, 3])\n",
    "converter = TFLiteConverter.from_concrete_functions([concrete_func])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Converting a Keras model (.h5 or hdf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a simple Keras model.\n",
    "x = [-1, 0, 1, 2, 3, 4]\n",
    "y = [-3, -1, 1, 3, 5, 7]\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    [tf.keras.layers.Dense(units=1, input_shape=[1])])\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "model.fit(x, y, epochs=50)\n",
    "\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 ) Converting a concrete function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "The following example shows how to convert a TensorFlow concrete function into a TensorFlow Lite FlatBuffer.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Construct a basic model.\n",
    "root = tf.train.Checkpoint()\n",
    "root.v1 = tf.Variable(3.)\n",
    "root.v2 = tf.Variable(2.)\n",
    "root.f = tf.function(lambda x: root.v1 * root.v2 * x)\n",
    "\n",
    "# Create the concrete function.\n",
    "input_data = tf.constant(1., shape=[1, 1])\n",
    "concrete_func = root.f.get_concrete_function(input_data)\n",
    "\n",
    "# Convert the model.\n",
    "#\n",
    "# `from_concrete_function` takes in a list of concrete functions, however,\n",
    "# currently only supports converting one function at a time. Converting multiple\n",
    "# functions is under development.\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end-to-end_mobilenet_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the MobileNet tf.keras model.\n",
    "model = tf.keras.applications.MobileNetV2(\n",
    "    weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the TensorFlow Lite model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Test the TensorFlow model on random input data.\n",
    "tf_results = model(tf.constant(input_data))\n",
    "\n",
    "# Compare the result.\n",
    "for tf_result, tflite_result in zip(tf_results, tflite_results):\n",
    "  np.testing.assert_almost_equal(tf_result, tflite_result, decimal=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
